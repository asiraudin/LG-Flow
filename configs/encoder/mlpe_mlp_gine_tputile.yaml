name: 'lpe'
num_vecs: 64
normalized_laplacian: no
normalize_eigenvecs: True
phi_model: 'mlp'
rho_model: 'gin'
pe_dim: 24
modulation: 0.05
ds_dim: 24

phi:
  num_layers: 2
  hidden_dim: 256
  bias: True
  dropout: 0.

rho:
  num_layers: 12
  embed_dim: 256